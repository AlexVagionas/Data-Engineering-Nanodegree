{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Airports\n",
    "## 1. ETL Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import first, count, when, col, create_map, lit, coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.0') \\\n",
    "    .appName('us_airports') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter AWS credentials below and uncomment if data are to be stored in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext \\\n",
    "#      ._jsc \\\n",
    "#      .hadoopConfiguration().set('fs.s3a.access.key', <enter access key>)\n",
    "\n",
    "# spark.sparkContext \\\n",
    "#      ._jsc \\\n",
    "#      .hadoopConfiguration().set('fs.s3a.secret.key', <enter secret key>)\n",
    "    \n",
    "# spark.sparkContext \\\n",
    "#      ._jsc \\\n",
    "#      .hadoopConfiguration().set('fs.s3a.endpoint', 's3.amazonaws.com')\n",
    "\n",
    "# spark.sparkContext \\\n",
    "#      ._jsc \\\n",
    "#      .hadoopConfiguration().set('mapreduce.fileoutputcommitter.algorithm.version', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function which creates a map between SAS labels and their values is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mapper(label):\n",
    "    with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "        sas_labels_content = f.read()\n",
    "\n",
    "    sas_labels_content = sas_labels_content.replace('\\t', '')\n",
    "\n",
    "    label_content = sas_labels_content[sas_labels_content.index(label):]\n",
    "    label_content = label_content[:label_content.index(';')].split('\\n')\n",
    "    label_content = [i.replace(\"'\", \"\") for i in label_content]\n",
    "\n",
    "    label_dict = [i.split('=') for i in label_content[1:]]\n",
    "    label_dict = dict([i[0].strip(), i[1].strip()] for i in label_dict if len(i) == 2)\n",
    "\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A {state_code: state} map is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94addr = code_mapper('i94addrl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states in map are formatted to comply with state names in other datasets.\n",
    "- 'DIST. OF' is replaced with 'District of'\n",
    "- 'S.', 'N.', 'W.' are replaced with 'South', 'North', 'West'\n",
    "- all states are capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(s):\n",
    "    s = s.replace('DIST. OF', 'District of') \\\n",
    "         .replace('S.', 'South') \\\n",
    "         .replace('N.', 'North') \\\n",
    "         .replace('W.', 'West')\n",
    "    return ' '.join([w.capitalize() if w != 'of' else w for w in s.split() ])\n",
    "\n",
    "i94addr = {k: format_state(v) for k, v in i94addr.items() if k != '99'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are read from .csv file in Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/airport-codes_csv.csv'\n",
    "df = spark.read.csv(path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new DataFrame is created containing the number of airports of each type in each state. Obviously, all closed and non-U.S. airports are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "    SELECT RIGHT(iso_region, LENGTH(iso_region) - 3) AS state_code, type, COUNT(*) AS airport_count\n",
    "      FROM airports\n",
    "     WHERE iso_country == \"US\" AND type != \"closed\"\n",
    "     GROUP BY iso_region, type\n",
    "     ORDER BY iso_region\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot is used to create columns for each individual type in state and when a certain type does not exists, null are replaced by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy('state_code').pivot('type').agg(first('airport_count')).orderBy('state_code').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'state' column is created by mapping state codes to states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_expr = create_map([lit(x) for x in chain(*i94addr.items())])\n",
    "\n",
    "df = df.withColumn('state', mapping_expr.getItem(col('state_code')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rows with invalid states are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save DataFrame as .parquet in s3a://us-immigration-project or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to S3 bucket\n",
    "# output_data = 's3a://us-immigration-project/'\n",
    "# df.write.parquet(os.path.join(output_data, 'state_airport'), 'overwrite')\n",
    "\n",
    "# for local write\n",
    "# df.write.parquet('./output/state_airport', 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- balloonport: long (nullable = true)\n",
      " |-- heliport: long (nullable = true)\n",
      " |-- large_airport: long (nullable = true)\n",
      " |-- medium_airport: long (nullable = true)\n",
      " |-- seaplane_base: long (nullable = true)\n",
      " |-- small_airport: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample DataFrame records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " state_code     | AK      \n",
      " balloonport    | 0       \n",
      " heliport       | 61      \n",
      " large_airport  | 2       \n",
      " medium_airport | 90      \n",
      " seaplane_base  | 146     \n",
      " small_airport  | 497     \n",
      " state          | Alaska  \n",
      "-RECORD 1-----------------\n",
      " state_code     | AL      \n",
      " balloonport    | 0       \n",
      " heliport       | 134     \n",
      " large_airport  | 4       \n",
      " medium_airport | 11      \n",
      " seaplane_base  | 7       \n",
      " small_airport  | 183     \n",
      " state          | Alabama \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 51 columns: 8\n"
     ]
    }
   ],
   "source": [
    "print('rows: {}'.format(df.count()), 'columns: {}'.format(len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for nulls in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------\n",
      " state_code     | 0   \n",
      " balloonport    | 0   \n",
      " heliport       | 0   \n",
      " large_airport  | 0   \n",
      " medium_airport | 0   \n",
      " seaplane_base  | 0   \n",
      " small_airport  | 0   \n",
      " state          | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Primary Key & Columns Related to Other Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total states in DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('state', 'state_code').distinct().orderBy('state').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display all states and state codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|state               |state_code|\n",
      "+--------------------+----------+\n",
      "|Alabama             |AL        |\n",
      "|Alaska              |AK        |\n",
      "|Arizona             |AZ        |\n",
      "|Arkansas            |AR        |\n",
      "|California          |CA        |\n",
      "|Colorado            |CO        |\n",
      "|Connecticut         |CT        |\n",
      "|Delaware            |DE        |\n",
      "|District of Columbia|DC        |\n",
      "|Florida             |FL        |\n",
      "|Georgia             |GA        |\n",
      "|Hawaii              |HI        |\n",
      "|Idaho               |ID        |\n",
      "|Illinois            |IL        |\n",
      "|Indiana             |IN        |\n",
      "|Iowa                |IA        |\n",
      "|Kansas              |KS        |\n",
      "|Kentucky            |KY        |\n",
      "|Louisiana           |LA        |\n",
      "|Maine               |ME        |\n",
      "|Maryland            |MD        |\n",
      "|Massachusetts       |MA        |\n",
      "|Michigan            |MI        |\n",
      "|Minnesota           |MN        |\n",
      "|Mississippi         |MS        |\n",
      "|Missouri            |MO        |\n",
      "|Montana             |MT        |\n",
      "|Nebraska            |NE        |\n",
      "|Nevada              |NV        |\n",
      "|New Hampshire       |NH        |\n",
      "|New Jersey          |NJ        |\n",
      "|New Mexico          |NM        |\n",
      "|New York            |NY        |\n",
      "|North Carolina      |NC        |\n",
      "|North Dakota        |ND        |\n",
      "|Ohio                |OH        |\n",
      "|Oklahoma            |OK        |\n",
      "|Oregon              |OR        |\n",
      "|Pennsylvania        |PA        |\n",
      "|Rhode Island        |RI        |\n",
      "|South Carolina      |SC        |\n",
      "|South Dakota        |SD        |\n",
      "|Tennessee           |TN        |\n",
      "|Texas               |TX        |\n",
      "|Utah                |UT        |\n",
      "|Vermont             |VT        |\n",
      "|Virginia            |VA        |\n",
      "|Washington          |WA        |\n",
      "|West Virginia       |WV        |\n",
      "|Wisconson           |WI        |\n",
      "|Wyoming             |WY        |\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('state', 'state_code').distinct().orderBy('state').show(51, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
